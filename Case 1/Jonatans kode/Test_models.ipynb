{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_vec shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "#load data from txt file with pandas\n",
    "data = pd.read_csv('case1Data.txt', sep=\", \", engine='python')\n",
    "y = data['y']\n",
    "\n",
    "X = pd.read_csv('case1Data_one_hot.csv').to_numpy()\n",
    "X_new = pd.read_csv('case1Data_Xnew_one_hot.csv').to_numpy()\n",
    "y_vec = y.values\n",
    "print('y_vec shape:', y_vec.shape)\n",
    "\n",
    "def transform_to_categorical_bool(inp_array):\n",
    "    N = inp_array.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(4):\n",
    "            idx = np.argmax(inp_array[i, 95+(j*5) : 95+(j+1)*5 ])\n",
    "            inp_array[i, 95+(j*5) : 95+(j+1)*5 ] = 0,0,0,0,0\n",
    "            inp_array[i, 95+(j*5) + idx] = 1\n",
    "    return inp_array\n",
    "\n",
    "def custom_scale(scaler, data_X, data_Xnew):\n",
    "    \"\"\" \n",
    "        Scale both X and Xnew based only on their continous values.\n",
    "    \"\"\"\n",
    "    scaler.fit(np.concatenate((data_X[:,:95], data_Xnew[:,:95]), axis = 0))\n",
    "    X_con, Xnew_con = scaler.transform(data_X[:,:95]), scaler.transform(data_Xnew[:,:95])\n",
    "    data_X_norm = np.concatenate((X_con, data_X[:,95:]), axis = 1)\n",
    "    data_Xnew_norm = np.concatenate((Xnew_con, data_Xnew[:,95:]), axis = 1)\n",
    "    all_data = np.concatenate((data_X_norm, data_Xnew_norm), axis = 0)\n",
    "\n",
    "    return scaler, data_X_norm, all_data\n",
    "\n",
    "def custom_transform(scaler, data):\n",
    "    return np.concatenate((scaler.transform(data[:,:95]), data[:,95:]), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try the setup for CV of imputation and model at the same time \n",
    "\n",
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Optimal neighbors:  8\n",
      "Optimal method: distance\n",
      "optimal alpha:  7.631578947368421\n",
      "Optimal RMSE 7.306692197378842\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge \n",
    "K = 10\n",
    "kf = KFold(n_splits=K)\n",
    "alphas = np.linspace(5, 10, 20)\n",
    "neighbors = range(5, 20, 1)\n",
    "methods = ['uniform', 'distance']\n",
    "errors = np.zeros((K, len(neighbors), len(methods), len(alphas)))\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(k)\n",
    "    scaler, Xtrain_norm, all_data = custom_scale(StandardScaler(), X[train_index], X_new) #Scale based on train data (leaving k'th fold out) and new data\n",
    "    Xtest_norm = custom_transform(scaler, X[test_index])\n",
    "    ytrain, ytest = y_vec[train_index], y_vec[test_index]\n",
    "\n",
    "    for i, neigh in enumerate(neighbors):\n",
    "        for m, method in enumerate(methods):\n",
    "            imputer = KNNImputer(n_neighbors=neigh, weights=method).fit(all_data)\n",
    "            #Impute data\n",
    "            Xtrain_imputed = transform_to_categorical_bool(imputer.transform(Xtrain_norm))\n",
    "            Xtest_imputed = transform_to_categorical_bool(imputer.transform(Xtest_norm))\n",
    "\n",
    "            for j, alpha in enumerate(alphas):\n",
    "                model = Ridge(alpha=alpha).fit(Xtrain_imputed, ytrain)\n",
    "                preds = model.predict(Xtest_imputed)\n",
    "                errors[k, i, m, j] = np.sum((preds - ytest)**2) #Sum up all squared error\n",
    "\n",
    "mean_error = errors.mean(axis = 0)\n",
    "idx = np.unravel_index(np.argmin(mean_error, axis=None), mean_error.shape)\n",
    "\n",
    "print('Optimal neighbors: ', neighbors[idx[0]])\n",
    "print('Optimal method:', methods[idx[1]])\n",
    "print('optimal alpha: ', alphas[idx[2]])\n",
    "print('Optimal RMSE', np.sqrt(mean_error[idx] / len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Optimal neighbors in imputation:  2\n",
      "Optimal method in imputation: uniform\n",
      "Optimal neighbors in prediction:  4\n",
      "Optimal method  in prediction: uniform\n",
      "Optimal RMSE 11.488682352926261\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "K = 10\n",
    "kf = KFold(n_splits=K)\n",
    "neighbors1 = range(1, 20, 1)\n",
    "methods1 = ['uniform', 'distance']\n",
    "\n",
    "neighbors2 = range(1, 20, 1)\n",
    "methods2 = ['uniform', 'distance']\n",
    "errors = np.zeros((K, len(neighbors1), len(methods1), len(neighbors2), len(methods2)))\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(k)\n",
    "    scaler, Xtrain_norm, all_data = custom_scale(StandardScaler(), X[train_index], X_new) #Scale based on train data (leaving k'th fold out) and new data\n",
    "    Xtest_norm = custom_transform(scaler, X[test_index])\n",
    "    ytrain, ytest = y_vec[train_index], y_vec[test_index]\n",
    "\n",
    "    for i1, neigh1 in enumerate(neighbors1):\n",
    "        for m1, method1 in enumerate(methods1):\n",
    "            imputer = KNNImputer(n_neighbors=neigh1, weights=method1).fit(all_data)\n",
    "            #Impute data\n",
    "            Xtrain_imputed = transform_to_categorical_bool(imputer.transform(Xtrain_norm))\n",
    "            Xtest_imputed = transform_to_categorical_bool(imputer.transform(Xtest_norm))\n",
    "\n",
    "            for i2, neigh2 in enumerate(neighbors2):\n",
    "                for m2, method2 in enumerate(methods2):\n",
    "                    knn_reg = KNeighborsRegressor(n_neighbors=neigh2, weights=method2).fit(Xtrain_imputed, ytrain)\n",
    "                    preds = knn_reg.predict(Xtest_imputed)\n",
    "                    errors[k, i1, m1, i2, m2] = np.sum((preds - ytest)**2) #Sum up all squared error\n",
    "\n",
    "mean_error = errors.mean(axis = 0)\n",
    "idx = np.unravel_index(np.argmin(mean_error, axis=None), mean_error.shape)\n",
    "\n",
    "print('Optimal neighbors in imputation: ', neighbors1[idx[0]])\n",
    "print('Optimal method in imputation:', methods1[idx[1]])\n",
    "print('Optimal neighbors in prediction: ', neighbors2[idx[2]])\n",
    "print('Optimal method  in prediction:', methods2[idx[3]])\n",
    "print('Optimal RMSE', np.sqrt(mean_error[idx] / len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Optimal neighbors:  9\n",
      "Optimal method: distance\n",
      "optimal alpha:  1.1315789473684212\n",
      "Optimal RMSE 6.378234813416324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "K = 10\n",
    "kf = KFold(n_splits=K)\n",
    "alphas = np.linspace(0.1, 5, 20)\n",
    "neighbors = range(5, 20, 1)\n",
    "methods = ['uniform', 'distance']\n",
    "errors = np.zeros((K, len(neighbors), len(methods), len(alphas)))\n",
    "\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(k)\n",
    "    scaler, Xtrain_norm, all_data = custom_scale(StandardScaler(), X[train_index], X_new) #Scale based on train data (leaving k'th fold out) and new data\n",
    "    Xtest_norm = custom_transform(scaler, X[test_index])\n",
    "    ytrain, ytest = y_vec[train_index], y_vec[test_index]\n",
    "\n",
    "    for i, neigh in enumerate(neighbors):\n",
    "        for m, method in enumerate(methods):\n",
    "            imputer = KNNImputer(n_neighbors=neigh, weights=method).fit(all_data)\n",
    "            #Impute data\n",
    "            Xtrain_imputed = transform_to_categorical_bool(imputer.transform(Xtrain_norm))\n",
    "            Xtest_imputed = transform_to_categorical_bool(imputer.transform(Xtest_norm))\n",
    "\n",
    "            for j, alpha in enumerate(alphas):\n",
    "                model = Lasso(alpha=alpha, max_iter=10000).fit(Xtrain_imputed, ytrain)\n",
    "                preds = model.predict(Xtest_imputed)\n",
    "                errors[k, i, m, j] = np.sum((preds - ytest)**2) #Sum up all squared error\n",
    "\n",
    "mean_error = errors.mean(axis = 0)\n",
    "idx = np.unravel_index(np.argmin(mean_error, axis=None), mean_error.shape)\n",
    "\n",
    "print('Optimal neighbors: ', neighbors[idx[0]])\n",
    "print('Optimal method:', methods[idx[1]])\n",
    "print('optimal alpha: ', alphas[idx[2]])\n",
    "print('Optimal RMSE', np.sqrt(mean_error[idx] / len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net regression (L1 + L2 combined regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "2\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "3\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "4\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "5\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "6\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "7\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "8\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "9\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "Optimal neighbors:  9\n",
      "Optimal method: distance\n",
      "optimal alpha:  1.188888888888889\n",
      "optimal l1 ratio:  1.0\n",
      "Optimal RMSE 6.363758869339667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet # Slow to run this CV loop!\n",
    "\n",
    "K = 10\n",
    "kf = KFold(n_splits=K)\n",
    "alphas = np.linspace(0.1, 5, 10)\n",
    "l1_ratios = np.linspace(0.1, 1, 10)\n",
    "neighbors = range(5, 21, 2)\n",
    "methods = ['uniform', 'distance']\n",
    "errors = np.zeros((K, len(neighbors), len(methods), len(alphas), len(l1_ratios)))\n",
    "c = 0\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(k)\n",
    "    scaler, Xtrain_norm, all_data = custom_scale(StandardScaler(), X[train_index], X_new) #Scale based on train data (leaving k'th fold out) and new data\n",
    "    Xtest_norm = custom_transform(scaler, X[test_index])\n",
    "    ytrain, ytest = y_vec[train_index], y_vec[test_index]\n",
    "\n",
    "    for i, neigh in enumerate(neighbors):\n",
    "        for m, method in enumerate(methods):\n",
    "            imputer = KNNImputer(n_neighbors=neigh, weights=method).fit(all_data)\n",
    "            #Impute data\n",
    "            Xtrain_imputed = transform_to_categorical_bool(imputer.transform(Xtrain_norm))\n",
    "            Xtest_imputed = transform_to_categorical_bool(imputer.transform(Xtest_norm))\n",
    "\n",
    "            for j, alpha in enumerate(alphas):\n",
    "                for l, l1 in enumerate(l1_ratios):\n",
    "                    c+=1\n",
    "                    if c % 500 == 0:\n",
    "                        print(c)\n",
    "                    model = ElasticNet(alpha=alpha, l1_ratio = l1, max_iter=10000, tol = 1e-3, warm_start=True).fit(Xtrain_imputed, ytrain)\n",
    "                    preds = model.predict(Xtest_imputed)\n",
    "                    errors[k, i, m, j, l] = np.sum((preds - ytest)**2) #Sum up all squared error\n",
    "\n",
    "mean_error = errors.mean(axis = 0)\n",
    "idx = np.unravel_index(np.argmin(mean_error, axis=None), mean_error.shape)\n",
    "\n",
    "print('Optimal neighbors: ', neighbors[idx[0]])\n",
    "print('Optimal method:', methods[idx[1]])\n",
    "print('optimal alpha: ', alphas[idx[2]])\n",
    "print('optimal l1 ratio: ', l1_ratios[idx[3]])\n",
    "print('Optimal RMSE', np.sqrt(mean_error[idx] / len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Optimal neighbors:  9\n",
      "Optimal method: uniform\n",
      "optimal non_zero coefs:  20\n",
      "Optimal RMSE 7.416760065567319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lars\n",
    "\n",
    "K = 10\n",
    "kf = KFold(n_splits=K)\n",
    "nonzero_coefs = range(5, 121, 5)\n",
    "neighbors = range(5, 20, 1)\n",
    "methods = ['uniform', 'distance']\n",
    "errors = np.zeros((K, len(neighbors), len(methods), len(nonzero_coefs)))\n",
    "c = 0\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(k)\n",
    "    scaler, Xtrain_norm, all_data = custom_scale(StandardScaler(), X[train_index], X_new) #Scale based on train data (leaving k'th fold out) and new data\n",
    "    Xtest_norm = custom_transform(scaler, X[test_index])\n",
    "    ytrain, ytest = y_vec[train_index], y_vec[test_index]\n",
    "\n",
    "    for i, neigh in enumerate(neighbors):\n",
    "        for m, method in enumerate(methods):\n",
    "            imputer = KNNImputer(n_neighbors=neigh, weights=method).fit(all_data)\n",
    "            #Impute data\n",
    "            Xtrain_imputed = transform_to_categorical_bool(imputer.transform(Xtrain_norm))\n",
    "            Xtest_imputed = transform_to_categorical_bool(imputer.transform(Xtest_norm))\n",
    "\n",
    "            for j, non_zero in enumerate(nonzero_coefs):\n",
    "                    model = Lars(n_nonzero_coefs=non_zero, eps = 1e-5).fit(Xtrain_imputed, ytrain)\n",
    "                    preds = model.predict(Xtest_imputed)\n",
    "                    errors[k, i, m, j] = np.sum((preds - ytest)**2) #Sum up all squared error\n",
    "\n",
    "mean_error = errors.mean(axis = 0)\n",
    "idx = np.unravel_index(np.argmin(mean_error, axis=None), mean_error.shape)\n",
    "\n",
    "print('Optimal neighbors: ', neighbors[idx[0]])\n",
    "print('Optimal method:', methods[idx[1]])\n",
    "print('optimal non_zero coefs: ', nonzero_coefs[idx[2]])\n",
    "print('Optimal RMSE', np.sqrt(mean_error[idx] / len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Optimal neighbors:  4\n",
      "Optimal method: uniform\n",
      "optimal alpha1:  1e-06\n",
      "optimal alpha2:  0.0001\n",
      "Optimal RMSE 7.34616877255294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "K = 10\n",
    "kf = KFold(n_splits=K)\n",
    "alphas1 = np.linspace(1e-6, 1e-4, 5)\n",
    "alphas2 = np.linspace(1e-6, 1e-4, 5)\n",
    "neighbors = range(1, 10, 1)\n",
    "methods = ['uniform', 'distance']\n",
    "errors = np.zeros((K, len(neighbors), len(methods), len(alphas1), len(alphas2)))\n",
    "c = 0\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(k)\n",
    "    scaler, Xtrain_norm, all_data = custom_scale(StandardScaler(), X[train_index], X_new) #Scale based on train data (leaving k'th fold out) and new data\n",
    "    Xtest_norm = custom_transform(scaler, X[test_index])\n",
    "    ytrain, ytest = y_vec[train_index], y_vec[test_index]\n",
    "\n",
    "    for i, neigh in enumerate(neighbors):\n",
    "        for m, method in enumerate(methods):\n",
    "            imputer = KNNImputer(n_neighbors=neigh, weights=method).fit(all_data)\n",
    "            #Impute data\n",
    "            Xtrain_imputed = transform_to_categorical_bool(imputer.transform(Xtrain_norm))\n",
    "            Xtest_imputed = transform_to_categorical_bool(imputer.transform(Xtest_norm))\n",
    "\n",
    "            for a1, alpha1 in enumerate(alphas1):\n",
    "                for a2, alpha2 in enumerate(alphas2):\n",
    "                    model = BayesianRidge(alpha_1=alpha1, alpha_2=alpha2).fit(Xtrain_imputed, ytrain)\n",
    "                    preds = model.predict(Xtest_imputed)\n",
    "                    errors[k, i, m, a1, a2] = np.sum((preds - ytest)**2) #Sum up all squared error\n",
    "\n",
    "mean_error = errors.mean(axis = 0)\n",
    "idx = np.unravel_index(np.argmin(mean_error, axis=None), mean_error.shape)\n",
    "\n",
    "print('Optimal neighbors: ', neighbors[idx[0]])\n",
    "print('Optimal method:', methods[idx[1]])\n",
    "print('optimal alpha1: ', alphas1[idx[2]])\n",
    "print('optimal alpha2: ', alphas2[idx[3]])\n",
    "print('Optimal RMSE', np.sqrt(mean_error[idx] / len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Optimal neighbors:  9\n",
      "Optimal method: distance\n",
      "optimal number of estimators:  10\n",
      "Optimal RMSE 10.739905716764365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "K = 10\n",
    "kf = KFold(n_splits=K)\n",
    "estimators = range(10, 100, 10)\n",
    "neighbors = range(1, 10, 1)\n",
    "methods = ['uniform', 'distance']\n",
    "errors = np.zeros((K, len(neighbors), len(methods), len(estimators)))\n",
    "c = 0\n",
    "for k, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(k)\n",
    "    scaler, Xtrain_norm, all_data = custom_scale(StandardScaler(), X[train_index], X_new) #Scale based on train data (leaving k'th fold out) and new data\n",
    "    Xtest_norm = custom_transform(scaler, X[test_index])\n",
    "    ytrain, ytest = y_vec[train_index], y_vec[test_index]\n",
    "\n",
    "    for i, neigh in enumerate(neighbors):\n",
    "        for m, method in enumerate(methods):\n",
    "            imputer = KNNImputer(n_neighbors=neigh, weights=method).fit(all_data)\n",
    "            #Impute data\n",
    "            Xtrain_imputed = transform_to_categorical_bool(imputer.transform(Xtrain_norm))\n",
    "            Xtest_imputed = transform_to_categorical_bool(imputer.transform(Xtest_norm))\n",
    "\n",
    "            for e, estimator in enumerate(estimators):\n",
    "                    model = RandomForestRegressor(n_estimators=estimator).fit(Xtrain_imputed, ytrain)\n",
    "                    preds = model.predict(Xtest_imputed)\n",
    "                    errors[k, i, m, e] = np.sum((preds - ytest)**2) #Sum up all squared error\n",
    "\n",
    "mean_error = errors.mean(axis = 0)\n",
    "idx = np.unravel_index(np.argmin(mean_error, axis=None), mean_error.shape)\n",
    "\n",
    "print('Optimal neighbors: ', neighbors[idx[0]])\n",
    "print('Optimal method:', methods[idx[1]])\n",
    "print('optimal number of estimators: ', estimators[idx[2]])\n",
    "print('Optimal RMSE', np.sqrt(mean_error[idx] / len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
